# Test programs

## Test answer's accuracy
### Test results
The test results are shown in `backend\app\tests\test_results` folder. There 
are 2 `jsonl` files in the folder:
* `test_reports.jsonl`: stores the result of the testing process in which the 
`test_chatbot_agent` assesses the accuracy of TUM chatbot's answer.
* `chat_history.jsonl`: stores the chat history that the `test_chatbot_agent` 
made with the TUM chatbot while testing. The chat history has more questions 
than the `test_reports.jsonl` because it starts working before the functionality 
to save data to `test_reports_agent` works. We regards all these information as 
valuable, so we save all of that for further investigation.

To analyse the performance of the TUM chatbot, we use `backend\app\tests\test_reports_analysis`. 
This file will count the number of times that TUM chatbot answers correctly, 
and the number of times its answer is wrong. Then it will calculate the accuracy
rate of the model.
### Test methods
We test the accuracy of chatbot's answers by using Google's Agent Development 
Kit (ADK) Agent. We call the agent as `test_chatbot_agent`. The noteworthy 
advantage of Google ADK Agent is that it can automatically select and run 
the functions (`tools`) given to it. The `test_chatbot_agent` has the following 
tools:
* `randomly_select_a_program`: randomly selects a program and returns all the 
program's information.
* `ask_llm_chatbot`: allows entering the prompt in the parameter, queries our 
TUM chatbot with the prompt, and returns the chatbot's answer.
* `save_test_report`: save the json generated by the `test_chatbot_agent` to a 
JSONL file and continuously update the file once the `test_chatbot_agent` call 
this tool.

The agent has the following tasks:
* Given Technical University of Munich (TUM) programs information, generate 
questions to ask the chatbot. The question should be clear enough so that there
is only one correct answer, and should give enough information to avoid any 
confusion for the chatbot.
* Ask our TUM chatbot
* Assess whether the chatbot's answer is correct
* Fill in a json template and save it to the `test_reports.jsonl` file

### Run tests
To run this test program, please follows the following steps:
1. Create `\backend\.env` file. The file should have the following information:
```env
# Application settings
APP_NAME="Voice Assistant Backend"
DEBUG=True

# Google Gemini API configuration
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash

# API authentication
API_KEY=your_secure_api_key_here

# Database configuration
DATABASE_URL=postgresql://username:password@localhost:5432/voice_assistant

# Redis configuration
REDIS_URL=redis://localhost:6379
# Or use separate configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
# Email settings
EMAIL_ADDRESS=the_address_to_send_email_from
EMAIL_PASSWORD=the_api_password_of_the_email
# The address can be the same
DEFAULT_EMAIL_RECIPIENT=the_address_to_receive_email_from
TUITION_EMAIL_RECIPIENT=the_address_to_receive_email_from
APPLICATION_EMAIL_RECIPIENT=the_address_to_receive_email_from
PROGRAM_EMAIL_RECIPIENT=the_address_to_receive_email_from
```
2. Create virtual environment
```bash
python -m venv venv
```

3. Install Dependencies

```bash
# Install dependencies
pip install -r requirements.txt
```
4. Actiate virtual environment
```bash
# On Window
venv\Scripts\activate

# On Linux/macOS
source venv/bin/activate
```
5. Go to `backend` directory
```bash
cd backend
```
6. Run test program:
```bash
python -m app.tests.test_tum_chatbot
```
7. See the results in `backend\app\tests\test_results` folder.
8. Analyse the TUM chatbot performance by running this:
```bash
python -m app.tests.test_reports_analysis
```
It should show the following information in the CLI
```bash
✅ Correct (True): 101
❌ Incorrect (False): 5
📊 Total: 106
📈 Accuracy rate: 0.9528301886792453%
```


## Test robustness against prompt attacks
### Introduction
As a chatbot for providing information about a university, we are aware of the 
importance of protecting the university reputation. Therefore, we conducted 
tests to see how the chatbot responses to different prompt attacks techniques. 
So far, the following prompt attacks techniques have been covered in our test
cases.
* Prompt injection: Cause chatbot response in an unintended way. For example, 
saying it is `EvilTUM` or adopting the pirate slang.
* Prompt leaking: Cause the chatbot to leak some of its previous prompts or instruction.
* Jailbreaking: Cover bad intention in a good attitude. For example, trying to 
persuade the chatbot to give advice on how to cheat in the exam by saying they 
are administrative staffs that want to know in order to prevent cheating behavior.

### Methods
We conduct the tests by creating Google ADK Agent which generates prompt attacks 
and queries our chatbot, then evaluating whether the chatbot is safe under that 
attacks or not.

We then modify our prompts to avoid such attacks and run these prompt attackes 
again. The result shows that these attacks are all prevented after the prompt 
is improved.

# Collected Data
You can see the data that we have collected until 15/7/2025 in the folder: `voiceAssistant\backend\app\tests\test_results_15072025`
* `accuracy_test_reports.jsonl`: shows the reports from the `test_chatbot_agent` 
when testing the accuracy of our LLM-based chatbot.
* `accuracy_test_chat_history.jsonl`: shows the chat history between 
`test_chatbot_agent` and our TUM chatbot during testing.
* `attack_test_reports.jsonl`: shows the reports from `prompt_attack_agent` when 
trying to attack our LLM-based chatbot.
* `prompt_attack_chat_history.jsonl`: shows that chat history between `prompt_attack_agent` 
and our TUM chatbot during testing.
* `attack_success_before.jsonl`: is the filtered version of the `attack_test_reports.jsonl` 
which shows only the cases that the attacks success.
* `attack_retry.jsonl`: stores the conversation after we improve the prompt and 
retries the attacks that success in `attack_success_before.jsonl`.

## Insights
From the collected data, we created the `prompt_attack_analysis.py` file, which 
shows the statistic of the `attack_success_before.jsonl`. The statistics are:
```
✅ Correct (True): 6
❌ Incorrect (False): 3
📊 Total: 9
📈 Accuracy rate: 0.6666666666666666%
PS C:\Users\Lenovo\Downloads\MasterProgram\FoundationsAndApplicationOfGenerativeAI\Project_2\voiceAssistant\backend> python -m app.tests.prompt_attack_analysis
--------------------- PROMPT INJECTION ----------------------------
✅ Number of attacks failed: 8
❌ Number of attacks succeeded: 5
📊 Total attacks: 13
📈 Safety rate: 0.6153846153846154
The cases that the prompt injection attack succeed is stored in file: ./app/tests/test_results_15072025/attack_success_before.jsonl
--------------------- PROMPT LEAKING ----------------------------
✅ Number of attacks failed: 2
❌ Number of attacks succeeded: 8
📊 Total attacks: 10
📈 Safety rate: 0.2
The cases that the prompt leaking attack succeed is stored in file: ./app/tests/test_results_15072025/attack_success_before.jsonl
--------------------- JAILBREAKING ----------------------------
✅ Number of attacks failed: 21
❌ Number of attacks succeeded: 0
📊 Total attacks: 21
📈 Safety rate: 1.0
The cases that the jailbreaking attack succeed is stored in file: ./app/tests/test_results_15072025/attack_success_before.jsonl
```


