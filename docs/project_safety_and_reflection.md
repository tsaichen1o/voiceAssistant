# go42TUM Project Safety and Reflection Report
## Introduction:
We developed a chatbot using Generative AI (GenAI) to help prospective students 
navigate TUM's application process. The chatbot is instructed to act as a 
professional academic advisor for the Technical University of Munich (TUM) 
and answer questions **only** based on the information available in the 
provided context from our TUM Knowledge Base. If the required information is 
not found in the context, it must clearly state that it cannot find the 
relevant information and must **not fabricate** an answer under any circumstances.

## Safety of GenAI: Considerations and Mitigation Strategies
### Safety Consideration
Safety is the most important factor in our chatbot design. The chatbot provides 
advice to applicants for TUM programs. If it provides incorrect information, 
it could prevent a program from reaching its intended audience or reduce the 
chances of attracting highly qualified applicants. Additionally, since the 
chatbot acts as a professional TUM academic advisor, any unintended behavior — 
such as responses triggered by prompt injection attacks—could potentially harm 
TUM’s reputation.
### Mitigation Strategies
We decided to rigorously test our chatbot's safety before releasing it to 
production. This includes testing the accuracy of its responses and its 
resilience against prompt injection attacks.

#### 1. Accuracy Testing

**Challenges**

We encountered several challenges while designing the accuracy tests:

* It is time-consuming and cognitively demanding for a human to read through 
the content of 180 TUM program websites, generate test questions, ask the 
chatbot, and then evaluate whether the answers are correct.
* Because of the large resource requirements, we cannot frequently generate 
a new series of questions manually.
* Since our chatbot is based on a Large Language Model (LLM), its answers are 
not always consistent. Therefore, traditional automated methods for validating 
correctness are not applicable.

**Methodologies**

To address these challenges, we developed an LLM-based testing agent, called 
the Test Chatbot Agent, which can automatically perform the following tasks:
* Generate TUM program-related questions along with their correct answers. 
These questions are designed to be objective and have only one correct answer. 
Example:
Question: "What is the standard duration of studies for the program Chemical 
Engineering - Bachelor of Science (B.Sc.)?"
Correct answer: "6 (fulltime)".
* Ask the question to our chatbot, compare its response with the correct 
answer, and evaluate whether the chatbot’s answer is accurate.
* Fill in a `test_report` JSON form and append it to a `.jsonl` file.

We used the **Google Agent Development Kit (ADK)** to build this Test Chatbot 
Agent. ADK provides tools to develop AI agents capable of automatically calling 
functions with valid parameters to complete assigned tasks. This is how our 
Test Chatbot Agent performs all testing steps without human involvement.

**Test Results**

According to the test report generated by the Test Chatbot Agent, our chatbot 
achieved an accuracy rate of 95%, correctly answering 101 out of 106 questions.

Since our testing method is also LLM-based, we manually verified the 
correctness of the report by examining the following:

* Question and Chatbot Answer: 
We verified that each question and the chatbot’s answer recorded in the report 
were accurate by checking the chat history. The result shows that:
    *  All questions were actually asked.
    * The chatbot's recorded responses matched the real responses.
    * In some cases, the Test Chatbot Agent shortened the answer to highlight 
    only key information. This was acceptable, as it did not affect the 
    correctness of the evaluation.
* Correct Answer:
We manually searched for the correct answers to ensure that the “correct answer” 
provided in the report was indeed accurate. The result confirmed their correctness.
* Assessment:
We checked whether the Test Chatbot Agent’s assessment of the chatbot’s answer 
was correct. We found:
    * **One** case where the chatbot's answer was actually correct, but it was 
    mistakenly assessed as incorrect.
    * **Two** cases where the chatbot answered, “I cannot find the relevant 
    information,” but the agent still marked those as incorrect. We believe 
    this is a misunderstanding, as the chatbot behaved correctly in those 
    situations.

## Lessons Learned and Reflections

Through the accuracy testing process, we found that our chatbot answered 
simple, objective questions correctly in the majority of cases (95%). However, 
around 3% of responses were incorrect. To improve trust and transparency, we 
did/ plan to:

* Add a warning message to the chatbot interface, stating that the chatbot's 
answers may be incorrect.
* Include a reference with each response so that users can verify the 
source and access further details.
